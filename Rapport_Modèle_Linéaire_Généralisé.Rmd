---
title: "Rapport Modèle Linéaire Généralisé"
author: "Olivier Berthier"
date: "2024-06-30"
output:
  html_document: default
  pdf_document: default
---


L'objet de ce rapport consiste à étudier un jeu de données d'entraînement sur les conditions météorologiques (à Bâle, en Suisse) et de faire des prédictions (classification) sur un jeu de test. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE)
```

```{r Packages, eval=FALSE, include=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com/"))
install.packages("ggcorrplot")
install.packages("biglm")
install.packages("formattable")
install.packages("rstanarm")
install.packages("bayesplot")
install.packages("car")
```

```{r Librairies, message=FALSE, warning=FALSE}
library(readr)
library (ggplot2)
library(ggcorrplot)
library(dplyr)
library(pROC)
library(tidyr)
library(boot)
library(caret)
library(formattable)
library(rstanarm)
library(bayesplot)
library(car)
```

## Importation et exploration des données

Importation du jeu de données d'entraînement dans le dataframe **d_train**. (Nous récapitulons en fin de devoir tous les noms et caractéristiques des dataframes et modèles utilisés dans le rapport ([ici](#ref1)).

```{r message=FALSE}
d_train <- read_csv("C:/Users/olivi/OneDrive - Université Paris-Dauphine/Documents ODD gram/Formation/Cours/Modèle Linéaire Généralisé/Projet/meteo.train.csv", 
    col_types = cols(...1 = col_skip(), Year = col_skip(), 
        Month = col_skip(), Day = col_skip(), 
        Hour = col_skip(), Minute = col_skip()))
```

Nous n'affichons pas ici un **summary()** du jeu de données ici à cause du nombre important de variables et de la place qu'occuperait la sortie.  
Les données comportent **1180** observations qui correspondent à autant de jours, du 2 juin 2010 au 18 juin 2018.    
Les données couvrent un jour sur deux dans cet intervalle (moins les 290 jours supprimés pour notre jeu de test).    
D'après le cahier des charges de l'exercice, l'objectif est de construire un modèle de classification permettant de prévoir s'il pleut le jour suivant en utilisant uniquement les données météo du jour. Nous écartons donc de nos variables explicatives la numérotation (**...1**), et les variables de date et d'heure (**Year**, **Month**, **Day**, **Hour** et **Minute**).  
Il reste 40 variables explicatives de type météorologique (humidité, couverture nuageuse, vent, température, etc.).  
Toutes les variables sont de type numérique. La valeur cible, **"pluie.demain"**, est de type binaire (logique).  
Le nombre de jours de pluie est supérieur au nombre de jours sans pluie, 601 avec pluie et 579 sans, soit un ratio  d'environ **0.51%** de jour avec pluie.  

Recherche données manquantes:
```{r Recherche donnéees manquantes}
missing_values <- colSums(is.na(d_train))
missing_columns <- missing_values[missing_values > 0]
print(names(missing_columns))
```
Aucune donnée manquante.

## Modèle naïf 

Commençons par un modèle incluant toutes les variables (**model_total**):
```{r model_total}
model_total <- glm(pluie.demain ~ ., data = d_train, family = binomial)
summary(model_total)
```
Nous constatons que beaucoup de variables ont des p-valeurs >5%. Ce modèle pourrait être largement amélioré. Nous le conservons cependant tel quel afin de comparer les performances d'un modèle "brut" avec des modèles plus sophistiqués.  

Évaluation de **model_total** par matrice de confusion:
```{r matrice de confusion model_total}
pred_prob <- predict(model_total, newdata = d_train, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE") # seuil de proba de 0.5
pred <- as.factor(pred)
pluie.demain <- as.factor(d_train$pluie.demain) 
# matrice de confusion
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```

Le modèle total prédit la bonne réponse dans 0.7398% das cas, cela correspond à la précision (Accuracy): (vrai positif + vrai négatif) / total).   
Pour avoir un ordre de grandeur, il prédit 873 bonnes réponses contre 307 erreurs.  
La sensibilité (Sensitivity) représente la proportion de jours pluvieux réels (cas positifs) que le modèle a correctement prédits: VP / (VP + FN). Le modèle prédit correctement un jour pluvieux dans 0.7671% des cas.    
(Note: cette mesure est particulièrement importante si la prédiction de la pluie est cruciale, par exemple pour éviter les inondations).  
La spécificité (0,7116%) représente la proportion de jours sans pluie réels (cas négatifs) que le modèle a correctement prédits (VN / (VN + FP)).  

Nous évaluons maintenant notre modèle total par cross-validation (10 plis) en calculant la valeur moyenne des erreurs de prédictions.  
Le modèle est entraîné 10 fois. À chaque itération, un pli différent est utilisé comme ensemble de test et les 9 plis restants comme ensemble d'entraînement.  
Cette méthode est intéressante dans notre étude car nous ne connaissons pas les valeurs de la variable cible de notre jeu de test, et ne pouvons donc pas tester notre modèle sur ce jeu de données. 
De plus, elle permet d'utiliser l'ensemble des valeurs du jeu d'entraînement.  
Prendre K=10 offre un bon compromis entre biais et variance. Un K plus petit peut introduire un biais plus élevé, et un K plus grand peut augmenter la variance et le coût computationnel (un grand K réduit également les risques de surapprentissage).  
(ici et pour la suite nous utilisons une "graine" afin d'obtenir des résultats "stables" qui faliciteront les comparaisons.)
```{r cv model_total}
set.seed(100)     # Fixer la graine pour la reproductibilité
cv_model <- cv.glm(d_train, model_total, K = 10)  # K = nombre de plis pour la validation croisée
delta_rounded <- round(cv_model$delta, 3)  # Arrondir les valeurs à trois chiffres après la virgule
print(delta_rounded)  # Afficher les valeurs 
```
La sortie donne deux valeurs, la première est l'estimation brute de l'erreur, la deuxième est l'estimation corrigée du biais introduit en n'utilisant pas la méthode "leave one out" (où K=n).  
Nous retenons ici et pour la suite la valeur corrigée, soit 0.189% d'erreur de prédiction.  

Calculons l'AUC de **model_total**.  
L'AUC (Area Under the Curve) évalue la performance du modèle en calculant l'aire sous la courbe ROC (Receiver Operating Characteristic). Nous reviendrons plus loin sur la courbe ROC.
```{r AUC model_total, message=FALSE, warning=FALSE}
pred_prob <- predict(model_total, newdata = d_train, type = "response") # Prédictions de probabilités sur les données d'entraînement
roc_model_total <- roc(d_train$pluie.demain, pred_prob)  # Calculer le ROC
auc_model_total <- auc(roc_model_total)   # Calculer l'AUC
print(auc_model_total)
```
Sachant que **0** correspond à un modèle qui prédit toujours incorrectement et **1** correspond à un modèle qui prédit toujours correctement, l'AUC de **model_total** semble assez élevé. Ces mesures de performances vont surtout nous servir ici à comparer entre eux les différents modèles.

## Élaboration automatique via la fonction step()
### Selon le critère AIC

Nous utilisons la fonction **step()** dans le but de réduire automatiquement le nombre de variables dans le modèle afin d'éviter le surapprentissage, et d'améliorer ainsi la généralisation sur de nouvelles données.
Nous utilisons en première approche le critère de sélection AIC (Akaike Information Criterion) qui vise à trouver un équilibre entre la précision de la modélisation et la complexité du modèle.
```{r model_step_AIC}
model_step_AIC <- step(glm(pluie.demain ~ ., data = d_train, family = binomial), direction = "both", trace = 0, k = 2)
summary (model_step_AIC)
```
Le modèle **model_step_AIC** retient 17 variables (23 ont été éliminées). Notons que certaines variables présentes affichent des p-vlaeurs >5% et que le modèle pourrait sans doute être amélioré.  
Nous le conservons tel quel afin de pouvoir comparer les différentes méthodes.  
Sans surprise, car c'est le critère de sélection, son AIC (1285.6) est meilleur que celui du modèle total (1321.5).  
Remarque: nous avons aussi essayé (toujours selon le critère AIC) la fonction **train()** du package **caret** qui utilise la cross-validation pour optimiser son modèle. Le modèle proposé était identique et retenait les mêmes variables et les mêmes coefficients.

Évaluation de **model_step_AIC** par matrice de confusion:  
(Pour ne pas surcharger le rapport, nous ne faisons pas apparaître les codes qui sont redondants: seul le nom du modèle est modifié par rapport au calcul de matrice de confusion précédent.)
```{r matrice de confusion model_step_AIC, echo=FALSE}
pred_prob <- predict(model_step_AIC, newdata = d_train, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(d_train$pluie.demain) 
# matrice de confusion
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```


876 bonnes réponses.

Évaluation de **model_step_AIC** par cross-validation (10 plis):
```{r cv model_step_AIC, echo=FALSE}
set.seed(100)     
# validation croisée
cv_model <- cv.glm(d_train, model_step_AIC, K = 10)  
delta_rounded <- round(cv_model$delta, 3)
print(delta_rounded)
```
Calculons l'AUC de **model_step_AIC**:
```{r AUC model_step_AIC, echo=FALSE, message=FALSE}
pred_prob <- predict(model_step_AIC, newdata = d_train, type = "response")
roc_model_step_AIC <- roc(d_train$pluie.demain, pred_prob)
auc_model_step_AIC <- auc(roc_model_step_AIC)
print(auc_model_step_AIC)
```
Le **model_step_AIC** fait un peu mieux que le modèle total pour 4 critères sur 6 (il est moins bon pour la spécificité et l'AUC, mais les différences sont négligeables).

### Selon le critère BIC
Modèle avec sélection BIC (Bayesian Information Criterion), obtenu avec k = log(n).  
Ce critère est plus parcimonieux que l'AIC. On considère généralement qu'il facilite l’interprétation (moins de variables) mais qu'il peut être moins bon en prédiction (perte d'information possible par élimination des variables).  
```{r model_step_BIC}
model_step_BIC <- step(glm(pluie.demain ~ . , data = d_train, family = binomial), direction = "both", trace = 0, k = log(1180))
summary (model_step_BIC)
```
Sans surprise, son AIC est moins bon que celui de **model_step_AIC**.  
Seulement 11 variables sont retenues, toutes avec une p-valeur <0.01%.  
9 d'entre elles étaient dèjà présentes dans **model_step_BIC**.

Les variables communes aux deux modèles sont: 

- Temperature.daily.mean..2.m.above.gnd.  
- Mean.Sea.Level.Pressure.daily.mean..MSL.  
- Temperature.daily.min..2.m.above.gnd.  
- Mean.Sea.Level.Pressure.daily.max..MSL.  
- Mean.Sea.Level.Pressure.daily.min..MSL.  
- Wind.Speed.daily.max..10.m.above.gnd.  
- Wind.Speed.daily.min..10.m.above.gnd.  
- Wind.Direction.daily.mean..900.mb.  
- Total.Cloud.Cover.daily.mean..sfc  

Évaluation de **model_step_BIC** par matrice de confusion:
```{r matrice de confusion model_step_BIC, echo=FALSE}
pred_prob <- predict(model_step_BIC, newdata = d_train, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(d_train$pluie.demain) 
# Calculer la matrice de confusion
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```
879 bonnes réponses.

Évaluation de **model_step_BIC** par cross-validation (10 plis):
```{r cv model_step_BIC, echo=FALSE, message=FALSE}
set.seed(100)     # Fixer la graine pour la reproductibilité
# validation croisée
cv_model <- cv.glm(d_train, model_step_BIC, K = 10)  # K = nombre de plis pour la validation croisée
# Arrondir les valeurs à trois chiffres après la virgule
delta_rounded <- round(cv_model$delta, 3)
print(delta_rounded)
```

Calculons l'AUC de **model_step_BIC**:
```{r AUC model_step_BIC, echo=FALSE, message=FALSE}
pred_prob <- predict(model_step_BIC, newdata = d_train, type = "response")
# Calculer l'AUC
roc_model_step_BIC <- roc(d_train$pluie.demain, pred_prob)
auc_model_step_BIC <- auc(roc_model_step_BIC)
print(auc_model_step_BIC)
```

Les performances de **model_step_BIC** sont très proches des deux autres (notons qu'il a la meilleure précision des trois).
Il prédit 4 faux positifs de plus que **model_step_AIC** mais 7 faux négatifs de moins.

## Sélection de variables par algorithme

Nous proposons ici un algorithme de sélection automatique afin de pallier le problème de multicolinéarité. Nous créons un algorithme qui élimine automatiquement les variables trop corrélées entre elles.  
Nous retenons 0.8 comme seuil de de corrélation (en valeur absolue).  
Ce code identifie les paires de variables dans le jeu de données au-dessus de ce seuil et élimine la variable avec la plus faible corrélation avec **pluie.demain**.  
La boucle est répétée jusqu'à ce qu'aucune paire de variables avec une corrélation >0.8 ne soit trouvée.  
Nous copions **d_train** dans un nouveau dataframe **d_train_algo** pour cet usage. 
```{r message=FALSE, warning=FALSE}
d_train_algo <- d_train  # nouveau dataframe pour effectuer la sélection
# Initialiser la boucle
variables_supprimees <- c()

repeat {
  # Calculer la matrice de corrélation
  cor_matrix <- cor(d_train_algo, use = "complete.obs")
  
  # Identifier les paires de variables avec une corrélation supérieure à 0.8
  high_cor_pairs <- which(abs(cor_matrix) > 0.8 & abs(cor_matrix) < 1, arr.ind = TRUE)
  high_cor_pairs <- high_cor_pairs[high_cor_pairs[, 1] < high_cor_pairs[, 2], , drop = FALSE]
  
  # Sortir de la boucle si aucune paire trouvée
  if (is.null(high_cor_pairs) || nrow(high_cor_pairs) == 0 || all(is.na(high_cor_pairs))) break
  
  # Trier les paires par corrélation décroissante
  cor_values <- cor_matrix[high_cor_pairs]
  cor_pairs <- data.frame(
    var1 = rownames(cor_matrix)[high_cor_pairs[, 1]],
    var2 = colnames(cor_matrix)[high_cor_pairs[, 2]],
    cor_value = cor_values
  )
  cor_pairs <- cor_pairs[order(abs(cor_pairs$cor_value), decreasing = TRUE), ]
  
  # Sélectionner la paire avec la corrélation la plus forte
  strongest_pair <- cor_pairs[1, ]
  var1 <- strongest_pair$var1
  var2 <- strongest_pair$var2
  
  # Calculer les corrélations avec pluie.demain
  cor_var1 <- cor(d_train_algo[[var1]], d_train_algo$pluie.demain)
  cor_var2 <- cor(d_train_algo[[var2]], d_train_algo$pluie.demain)
  
  # Afficher les corrélation
  cat("Corrélation entre", var1, "et", var2, ":", strongest_pair$cor_value, "\n")
  cat("Corrélation de", var1, "avec pluie.demain:", cor_var1, "\n")
  cat("Corrélation de", var2, "avec pluie.demain:", cor_var2, "\n")
  
  if (abs(cor_var1) < abs(cor_var2)) {
    d_train_algo <- d_train_algo %>% select(-var1)
    variables_supprimees <- c(variables_supprimees, var1)
    cat("Suppression de la variable:", var1, "\n\n")
  } 
  else {
    d_train_algo <- d_train_algo %>% select(-var2)
    variables_supprimees <- c(variables_supprimees, var2)
    cat("Suppression de la variable:", var2, "\n\n")
  }
}
cat("Nombre de variables supprimées :", length(variables_supprimees), "\n")
cat("Variables supprimées :", paste(variables_supprimees, collapse = ", "), "\n")

```
17 variables ont été supprimées selon nos critères, il reste donc 23 variables explicatives dans **d_train_algo**.

Nous constatons que notre méthode de sélection a écarté des variables retenues par la méthode step avec critère AIC.   

Variables présentes dans **model_step_AIC** mais éliminées par notre algorithme:  

- Wind.Speed.daily.mean..80.m.above.gnd.  
- Wind.Gust.daily.mean..sfc.  
- Mean.Sea.Level.Pressure.daily.max..MSL.  
- Total.Cloud.Cover.daily.min..sfc.  
- Wind.Speed.daily.max..10.m.above.gnd.  
- Wind.Speed.daily.min..10.m.above.gnd.  
- Wind.Speed.daily.min..80.m.above.gnd.  

Affichons le corrélogramme des variables restantes:
```{r fig.height=9, fig.width=9}
corr <- round(cor(d_train_algo), 2)
ggcorrplot(corr,
           method = "circle",
           lab = TRUE,
           lab_size = 2,
           show.diag = FALSE,       
           tl.cex = 3)
```

Il ne reste en effet aucune corrélation supérieure à 0.8.  

Modèle utilisant toutes les variabes restantes et calcul du VIF:
```{r  model _algo complet}
model_algo <- glm(pluie.demain ~ ., data = d_train_algo, family = binomial)
summary (model_algo)
vif_values <- vif(model_algo)  # calculer les VIF
print(vif_values)
```
Aucune valeur de VIF supérieure à 10 n'est constatée sur le modèle utilisant toutes les variables retenues.  
Le VIF (Variance Inflation Factor) est une mesure utilisée pour détecter la présence de multicolinéarité entre les variables explicatives.  
(On aurait pu aussi sélectionner nos variables avec ce critère plutôt qu'avec la corrélation de 0.8, en prenant par exemple 10 comme valeur seuil du VIF).  

Notre modèle avec 23 variables explicatives peut être largement amélioré (présence de trop fortes p-valeurs).  

Nous créons une boucle pour sélectionner automatiquement la variable du modèle qui a la plus forte p-valeur et vérifions par ANOVA que le modèle mis à jour sans cette variable n'affecte pas significativement l'ajustement du modèle.  
Nous arrêtons la boucle quand aucune variable restante n'a une p-valeur >0.1.
```{r Élimination par p-valeur et vérification par ANOVA}
repeat {
  # Obtenir un résumé du modèle
  summary_model <- summary(model_algo)
  
  # Extraire les p-values des coefficients et leurs noms de variable
  coef_data <- data.frame(
    Variable = rownames(summary_model$coefficients),
    P_Value = summary_model$coefficients[, 4]
  )
  
  # Trouver la plus forte p-value
  max_p_value <- max(coef_data$P_Value)
  
  # Sortir de la boucle si la plus forte p-value est inférieure à 0.1
  if (max_p_value < 0.10) {
    break
  }
  
  # Extraire le nom de la variable correspondant à la plus forte p-value
  variable_max_p <- coef_data$Variable[which.max(coef_data$P_Value)]
  
  # Afficher la plus forte p-value et le nom de la variable
  cat("\nLa plus forte p-value parmi les coefficients du modèle est :", max_p_value, "\n")
  cat("Variable correspondante :", variable_max_p, "\n")
  
  # Mettre à jour le modèle en supprimant la variable avec la plus forte p-value
  model_algo <- update(model_algo, paste(". ~ . -", variable_max_p))
  
  # Comparaison des deux modèles avec l'ANOVA
  anova_result <- anova(update(model_algo, paste(". ~ . +", variable_max_p)), model_algo, test = "LRT")
  
  # Afficher les résultats de l'ANOVA
  print(anova_result)
}
```
Les ANOVA successives confirment que le retrait de la variable à chaque étape n'affecte pas significativement l'ajustement du modèle.  

Affichons le modèle ainsi retenu:
```{r model_algo}
summary(model_algo)
```
Notre méthode ne retient que 10 variables explicatives.

Évaluation de **model_algo** par matrice de confusion:
```{r matrice de confusion model_algo, echo=FALSE}
pred_prob <- predict(model_algo, newdata = d_train_algo, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(d_train_algo$pluie.demain) # 
# Calculer la matrice de confusion
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```
873 bonnes réponses.

Évaluation de **model_algo** par cross-validation (10 plis):
```{r cv model_algo, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(100) 
# validation croisée
cv_model <- cv.glm(d_train_algo, model_algo, K = 10)  # K = nombre de plis pour la validation croisée

# Arrondir les valeurs à trois chiffres après la virgule
delta_rounded <- round(cv_model$delta, 3)
print(delta_rounded)
```
Calculons l'AUC de **model_algo**:
```{r AUC model_algo, echo=FALSE, message=FALSE, warning=FALSE}
# Faire des prédictions de probabilités sur les données d'entraînement (ou de test)
pred_prob <- predict(model_algo, newdata = d_train_algo, type = "response")

# Calculer l'AUC
roc_model_algo <- roc(d_train$pluie.demain, pred_prob)

# Afficher l'AUC
auc_model_algo <- auc(roc_model_algo)
print(auc_model_algo)
```
Le **model_algo** obtient le même nombre de bonnes réponses que le modèle total avec quatre fois moins de variables explicatives.
Sur les autres critères de performance, il est assez comparable aux autres modèles développés jusqu'ici.

## Sélection des variables "à la main"

Pour le moment, nos choix se sont faits "à l'aveugle", c'est-à-dire sans analyser directement les variables et les corrélations entre elles.   
Voyons si nous pouvons faire mieux en les analysant plus précisément.

Commençons par afficher le correlogramme de l'ensemble des données.
```{r matrice de corrélation sur d_train, fig.height=9, fig.width=9}
corr <- round(cor(d_train), 1)
ggcorrplot(corr, 
           lab = FALSE,
           show.diag=FALSE,
           tl.cex = 4,
           title= "         Corrélogramme de toutes les variables")
```


Le trop grand nombre de variables rend le corrélogramme difficilement exploitable. Il permet cependant de constater que nombre de variables sont très corrélées entre elles. Nous voyons également que la variable d’intérêt (dernière colonne) n'a de forte corrélation avec aucune des variables explicatives (ce qui peut expliquer que la fonction **step()** a retourné des modèles assez différents selon le critère demandé, qui ne sont d'ailleurs pas emboîtés).

Nous allons analyser les variables par groupes et les sélectionner "manuellement".  
Cette méthode est plus laborieuse mais elle serait sans doute à privilégier pour un spécialiste en météorologie afin qu'il garde la main sur l'information retenue ou rejetée.  
Nous divisons notre analyse en trois groupes, les variables de type **"Wind"**, celles de type **"Cloud"** et le groupe des variables n’appartenant à aucun des deux groupes précédants que nous nommons **"Reliquat"**.

### Variables "Wind"
```{r fig.height=9, fig.width=9}
d_wind <- select(d_train, contains("Wind"), pluie.demain)

corr <- round(cor(d_wind), 2)

ggcorrplot(corr,
           method = "circle",
           lab_size = 2.5,
           lab = TRUE,
           show.diag=FALSE,
           tl.cex = 7)
```


Premier constat, toutes les corrélations sont positives. Plus de vent tend en général à augmenter la probabilité de pluie le jour suivant.    
Nous voyons aussi que les plus fortes corrélations sont entre les variables explicatives elles-mêmes, beaucoup plus qu'avec notre variable cible (malheureusement).

Après analyse du corrélogramme, nous retenons la variable **"Wind.Gust.daily.max..sfc."** de corrélation 0.28 avec la variable d’intérêt. **"Wind.Gust.daily.max..sfc."** est aussi très corrélée avec d'autres variables qui ont elles-mêmes une forte corrélation avec **"pluidemain"** .  
Nous retenons aussi **"Wind.Direction.daily.mean..900.mb."** de corrélation 0.24, et peu corrélée avec **"Wind.Gust.daily.max..sfc."** (0.23).

### Variables "Cloud"

```{r correlation wind, fig.height=10, fig.width=10}
d_cloud <- d_train %>%
  select(contains("Cloud"), pluie.demain)

corr <- round(cor(d_cloud), 2)
ggcorrplot(corr,
           method = "circle",
           lab_size = 3,
           lab = TRUE,
           show.diag=FALSE,
           tl.cex = 7)
```
Les corrélations avec la variable d’intérêt sont globalement plus fortes qu'elles ne l'étaient avec le vent (ce qui est assez intuitif).

Nous réunissons certaines variables au sein de nouvelles variables afin de simplifier et de tenter d'augmenter les corrélations avec la variable cible:
```{r correlation cloud, fig.height=5, fig.width=5}
d_cloud_2 <- d_cloud
d_cloud_2 <- d_cloud_2 %>%
  mutate(Medium.Cloud = rowMeans(select(., Medium.Cloud.Cover.daily.max..mid.cld.lay., Medium.Cloud.Cover.daily.mean..mid.cld.lay., )))  %>%
 select(-Medium.Cloud.Cover.daily.max..mid.cld.lay., -Medium.Cloud.Cover.daily.mean..mid.cld.lay.) %>%
  mutate(Low.Cloud = rowMeans(select(., Low.Cloud.Cover.daily.max..low.cld.lay., Low.Cloud.Cover.daily.mean..low.cld.lay., ))) %>%
 select(-Low.Cloud.Cover.daily.max..low.cld.lay., -Low.Cloud.Cover.daily.mean..low.cld.lay.)%>%
  mutate(High.Cloud = rowMeans(select(., High.Cloud.Cover.daily.mean..high.cld.lay., High.Cloud.Cover.daily.max..high.cld.lay., )))%>%
 select(-High.Cloud.Cover.daily.mean..high.cld.lay., -High.Cloud.Cover.daily.max..high.cld.lay.)%>%
  mutate(Total.Cloud = rowMeans(select(., Total.Cloud.Cover.daily.max..sfc., Total.Cloud.Cover.daily.mean..sfc., )))%>%
select(-Total.Cloud.Cover.daily.max..sfc., -Total.Cloud.Cover.daily.mean..sfc.)
 
d_cloud_2 <- d_cloud_2[, !grepl("min", names(d_cloud_2))] # Elimine les variables de type contenant "min" dans leur nom.

corr <- round(cor(d_cloud_2), 2)
ggcorrplot(corr,
           method = "circle",
           lab_size = 3,
           lab = TRUE,
           tl.cex = 8)
```


Nous avons supprimé toutes les valeurs "min" (peu corrélées) et moyenné les valeurs "high" au sein de **"High.Cloud"**, moyenné les valeurs "total" au sein de **"Total.Cloud"**, les variables "low" dans **"Low.Cloud"** et les variables "medium" dans **"Medium.cloud"**.  
Cette démarche vise à simplifier le groupe mais aussi à augmenter la corrélation avec la variable d’intérêt.

Après analyse du corrélogramme mis à jour, nous ne retenons pas **"Low.Cloud"**, qui est la moins corrélée avec notre variable cible (0.25) et qui est aussi très corrélée avec **"Total.Cloud"** (0.87 entre elles).

### Variables restantes

```{r d_reliq, fig.height=10, fig.width=10}
d_reliq <- d_train %>%
  select( -contains(c("Wind","Cloud","numb","Year","Month", "Day")),pluie.demain)

corr <- round(cor(d_reliq), 2)
ggcorrplot(corr,
           method = "circle",
           lab_size = 3,
           lab = TRUE,
           show.diag=FALSE,
           tl.cex = 7)
```
Nous effectuons le même travail de sélection (favoriser les meilleures corrélations avec **pluie.demain**, et éviter une trop forte multicolinéarité).

Liste finale des variables retenues:

- Wind.Direction.daily.mean..900.mb.
- Wind.Gust.daily.max..sfc.
- Medium.Cloud
- High.Cloud
- Total.Cloud
- Total.Precipitation.daily.sum..sfc.
- Sunshine.Duration.daily.sum..sfc.
- Mean.Sea.Level.Pressure.daily.min..MSL.
- Temperature.daily.min..2.m.above.gnd.


### Modèle avec les variables retenues

Créons un nouveau dataframe **d_train_new** qui ajoute nos nouvelles variables retenues à **d_train**:
```{r d_train_new}
d_train_new <- d_train %>%
  mutate(Medium.Cloud = rowMeans(select(., Medium.Cloud.Cover.daily.max..mid.cld.lay., Medium.Cloud.Cover.daily.mean..mid.cld.lay., )))  %>%
 select(-Medium.Cloud.Cover.daily.max..mid.cld.lay., -Medium.Cloud.Cover.daily.mean..mid.cld.lay.) %>%
  mutate(Low.Cloud = rowMeans(select(., Low.Cloud.Cover.daily.max..low.cld.lay., Low.Cloud.Cover.daily.mean..low.cld.lay., ))) %>%
 select(-Low.Cloud.Cover.daily.max..low.cld.lay., -Low.Cloud.Cover.daily.mean..low.cld.lay.)%>%
  mutate(High.Cloud = rowMeans(select(., High.Cloud.Cover.daily.mean..high.cld.lay., High.Cloud.Cover.daily.max..high.cld.lay., )))%>%
 select(-High.Cloud.Cover.daily.mean..high.cld.lay., -High.Cloud.Cover.daily.max..high.cld.lay.)%>%
  mutate(Total.Cloud = rowMeans(select(., Total.Cloud.Cover.daily.max..sfc., Total.Cloud.Cover.daily.mean..sfc., )))%>%
select(-Total.Cloud.Cover.daily.max..sfc., -Total.Cloud.Cover.daily.mean..sfc.)
```
Après essai du modèle (non représenté ici) sur l'ensemble des variables de **d_train_new**, nous ne retenons pas **Total.Precipitation.daily.sum..sfc.** (p-valeur de 0.84107).

Nous obtenons finalement le modèle suivant:
```{r model_select final}
model_select <- glm(pluie.demain ~  Wind.Gust.daily.max..sfc.+ Wind.Direction.daily.mean..900.mb.+Medium.Cloud + High.Cloud + Total.Cloud + Sunshine.Duration.daily.sum..sfc.+ Mean.Sea.Level.Pressure.daily.min..MSL. +Temperature.daily.min..2.m.above.gnd., data = d_train_new, family = binomial)
summary (model_select)
```
Évaluation de **model_select** par matrice de confusion:
```{r matrice de confusion model_select, echo=FALSE}
pred_prob <- predict(model_select, newdata = d_train_new, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(d_train_new$pluie.demain) # 
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```
Évaluation de **model_select** par cross-validation (10 plis):
```{r cv model_select, echo=FALSE}
set.seed(100)
# Définir la fonction pour la validation croisée
cv_model <- cv.glm(d_train_new, model_select, K = 10)  # K = nombre de plis pour la validation croisée

# Arrondir les valeurs à trois chiffres après la virgule
delta_rounded <- round(cv_model$delta, 3)

# Afficher les valeurs arrondies
print(delta_rounded)
```
Calculons l'AUC de **model_step_AIC**:
```{r AUC model_select, echo=FALSE, message=FALSE}
pred_prob <- predict(model_select, newdata = d_train_new, type = "response")
roc_model_select <- roc(d_train_new$pluie.demain, pred_prob)
auc_model_select <- auc(roc_model_select)
print(auc_model_select)
```
Avec 870 bonnes réponses, **model_select** obtient globalement de moins bonnes performances que les autres modèles étudiés jusqu'ici. 
Mais il ne démérite pas concernant les autres critères de performance malgré sa parcimonie assez poussée (8 variables).  
Cette parcimonie rend plus facile l'ajout d'interactions, d'un point de vue computationnel, mais aussi en cas de recherche de modèle explicatif.

### model_select avec interactions

Sélectionnons les interactions avec la fonction **step()**:
```{r model_select avec interactions, message=FALSE, warning=FALSE}
formula <- as.formula("pluie.demain ~ (Wind.Gust.daily.max..sfc. + Wind.Direction.daily.mean..900.mb. + 
    Medium.Cloud + High.Cloud + Total.Cloud + Sunshine.Duration.daily.sum..sfc. + 
    Mean.Sea.Level.Pressure.daily.min..MSL. + Temperature.daily.min..2.m.above.gnd.)^2")

# Créer un modèle provisoire avec toutes les interactions
model_prov <- glm(formula, data = d_train_new, family = binomial)

# Sélectionner modèle par méthode step selon critère AIC
model_select_int <- step(model_prov, direction = "both", trace = 0)
summary (model_select_int)
```
7 Interactions ont été retenues.
L'AIC a baissé assez largement: 1247.6 contre 1315.7 sans interactions. Il est même le plus faible rencontré jusqu'ici.

Évaluation de **model_select_int** par matrice de confusion:
```{r matrice de confusion model_select_int, echo=FALSE}
pred_prob <- predict(model_select_int, newdata = d_train_new, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(d_train_new$pluie.demain) # 
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```
Évaluation de **model_select_int** par cross-validation (10 plis):
```{r cv model_select_int, echo=FALSE}
set.seed(100)
# validation croisée
cv_model <- cv.glm(d_train_new, model_select_int, K = 10)  # K = nombre de plis pour la validation croisée

delta_rounded <- round(cv_model$delta, 3)
print(delta_rounded)
```
Calculons l'AUC de **model_select_int**:
```{r AUC model_select_int, echo=FALSE, message=FALSE}
pred_prob <- predict(model_select_int, newdata = d_train_new, type = "response")
roc_model_select_int <- roc(d_train_new$pluie.demain, pred_prob)
auc_model_select_int <- auc(roc_model_select_int)
print(auc_model_select_int)
```
Le **model_select_int** donne la bonne réponse 877 fois sur 1180 (le meilleur score jusqu'ici) et obtient de bonnes performances, comparativement aux autres modèles, sur tous nos critères.

## Analyse par composantes principales 

Essayons une approche par composantes principales.
L'avantage est qu'ici nous n'avons pas à nous soucier de la multicolinéarité, les composantes principales étant indépendantes entre elles (corrélation égale à 0 pour chaque paire de composantes).
Effectuons l'ACP sur toutes les variables et évaluons un modèle complet:
```{r model_ACP_complet}
# Effectuer l'ACP en normalisant les données (scale = TRUE)
pca_result <- prcomp(d_train[, -which(names(d_train) == "pluie.demain")], scale. = TRUE)
summary(pca_result)

# Utiliser toutes les composantes principales pour la régression logistique
X_pca <- predict(pca_result, newdata = d_train[, -which(names(d_train) == "pluie.demain")])

# Combiner les composantes principales avec la variable cible
data_pca <- data.frame(X_pca, pluie.demain = d_train$pluie.demain)

# Modèle de régression logistique avec toutes les composantes principales
model_ACP_complet <- glm(pluie.demain ~ ., data = data_pca, family = binomial)

summary(model_ACP_complet)
```
Évaluation de **model_ACP_complet** par matrice de confusion:
```{r matrice de confusion model_ACP_complet, echo=FALSE}
pred_prob <- predict(model_ACP_complet, newdata = data_pca, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(data_pca$pluie.demain) # 
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```
Évaluation de **model_ACP_complet** par cross-validation (10 plis):
```{r cv model_ACP_complet, echo=FALSE}
set.seed(100)
# validation croisée
cv_model <- cv.glm(data_pca, model_ACP_complet, K = 10)  # K = nombre de plis pour la validation croisée

delta_rounded <- round(cv_model$delta, 3)
print(delta_rounded)
```
Calculons l'AUC de **model_ACP_complet**:
```{r AUC model_ACP_complet, echo=FALSE, message=FALSE}
pred_prob <- predict(model_ACP_complet, newdata = data_pca, type = "response")
roc_model_ACP_complet <- roc(data_pca$pluie.demain, pred_prob)
auc_model_ACP_complet <- auc(roc_model_ACP_complet)
print(auc_model_ACP_complet)
```

Nous constatons que pour tous nos critères, les résultats sont rigoureusement identiques à ceux du modèle total sur les variables "brutes" (**model_total**).
Nous l'expliquons par le fait nous avons la même "quantité d'information" dans ces deux modèles.

### Avec sélection des composantes principales

Si les deux modèles avec toutes les variables (avec et sans ACP) sont identiques, nous pouvons utiliser nos composantes principales en sélectionnant les premières, celles qui expliquent le mieux la variance.  
Utilisons la méthode du coude pour sélectionner nos CP, avec 90% de la variance expliquée comme seuil:
```{r Méthode du coude}
explained_variance <- pca_result$sdev^2 / sum(pca_result$sdev^2)
cumulative_explained_variance <- cumsum(explained_variance)

# Tracer le graphique 
plot(cumulative_explained_variance, type = "b", pch = 19, xlab = "Nombre de composantes principales", ylab = "Variance expliquée cumulée", main = "Méthode du coude")

# ligne horizontale 0.9
abline(h = 0.9, col = "red", lty = 2) 
```


Nous retenons les 13 premières composantes principales.
Élaborons un modèle à partir de ces 13 variables en utilisant la fonction **step()** avec interactions.

Note: nous avons essayé un modèle utilisant directement les 13 composantes dans un modèle sans sélection de composantes ni d'interactions, les performances sont décevantes: 0.7263 en précision et AIC de 1406.6. Nous ne le présentons pas ici.

### ACP avec interactions

```{r model_ACP_13_int, warning=FALSE}
# Capturer l'horodatage de début
start_time <- Sys.time()

# Sélectionner les 13 premières CP
X_pca_13 <- X_pca[, 1:13] 

# Créer un nouveau data.frame avec les composantes principales et la variable cible
data_pca_13 <- data.frame(X_pca_13, pluie.demain = d_train$pluie.demain)

# Définir la formule initiale avec les interactions entre les 13 premières composantes principales
formula <- as.formula(paste("pluie.demain ~ (", paste(names(data_pca_13)[-ncol(data_pca_13)], collapse = " + "), ")^2"))

# Créer le modèle initial avec toutes les interactions
model_initial <- glm(formula, data = data_pca_13, family = binomial)

# Sélectionner le meilleur modèle par méthode step avec critère AIC, en incluant les interactions
model_ACP_13_int <- step(model_initial, direction = "both", trace = 0)

# Afficher un résumé du modèle sélectionné
summary(model_ACP_13_int)

# Capturer l'horodatage de fin
end_time <- Sys.time()
# Calculer la différence en temps
time_taken <- end_time - start_time
# Afficher le temps de calcul
print(time_taken)
```
Évaluation de **model_ACP_13_int** par matrice de confusion:
```{r matrice de confusion model_ACP_13_int, echo=FALSE}
pred_prob <- predict(model_ACP_13_int, newdata = data_pca_13, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(data_pca_13$pluie.demain) # 
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```


Évaluation de **model_ACP_13_int** par cross-validation (10 plis):
```{r cv model_ACP_13_int, echo=FALSE, warning=FALSE}
set.seed(100)
# validation croisée
cv_model <- cv.glm(data_pca_13, model_ACP_13_int, K = 10)  # K = nombre de plis pour la validation croisée
delta_rounded <- round(cv_model$delta, 3)
print(delta_rounded)
```
Calculons l'AUC de **model_ACP_13_int**:
```{r AUC model_ACP_13_int, echo=FALSE, message=FALSE, warning=FALSE}
pred_prob <- predict(model_ACP_13_int, newdata = data_pca_13, type = "response")
roc_model_ACP_13_int <- roc(data_pca_13$pluie.demain, pred_prob)
auc_model_ACP_13_int <- auc(roc_model_ACP_13_int)
print(auc_model_ACP_13_int)
```
Avec 914 bonnes réponses, **roc_model_ACP_13_int** obtient le meilleur score et améliore de 35 points celui de **model_step_BIC** (879).
Il obtient la meilleure performance pour tous les critères sauf pour l'AIC et pour l'estimation de l'erreur de prédiction par cross-validation (respectivement 1276.3 et 0.185).

### ACP avec citère AIC
Appliquons une sélection par la fonction **step()** (critère AIC) sur le dataframe complet des composantes principales:

```{r}
model_ACP_AIC <- step(glm(pluie.demain ~ . , data = data_pca, family = binomial), direction = "both", trace = 0)
summary(model_ACP_AIC)
```
14 composantes sont retenues.

Il est intéressant de noter que si 4 des 5 premières composantes sont retenues (celles expliquant le mieux la variance), des composantes principales parmi les dernières sont elles aussi retenues, notamment PC40 qui a aussi le plus gros coefficient.  

Évaluation de **model_ACP_AIC** par matrice de confusion:
```{r matrice de confusion model_ACP_AIC, echo=FALSE}
pred_prob <- predict(model_ACP_AIC, newdata = data_pca, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(data_pca$pluie.demain) # 
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```
Évaluation de **model_ACP_AIC** par cross-validation (10 plis):
```{r cv model_ACP_AIC, echo=FALSE, message=FALSE}
set.seed(100)
# validation croisée
cv_model <- cv.glm(data_pca, model_ACP_AIC, K = 10)  # K = nombre de plis pour la validation croisée
delta_rounded <- round(cv_model$delta, 3)
print(delta_rounded)
```
Calculons l'AUC de **model_ACP_AIC**:
```{r AUC model_ACP_AIC}
set.seed(100)
# Faire des prédictions de probabilités sur les données d'entraînement (ou de test)
pred_prob <- predict(model_ACP_AIC, newdata = data_pca, type = "response")
# Calculer l'AUC
roc_model_ACP_AIC <- roc(data_pca$pluie.demain, pred_prob)
# Afficher l'AUC
auc_model_ACP_AIC <- auc(roc_model_ACP_AIC)
print(auc_model_ACP_AIC)
```
Avec 882 bonnes réponses, **model_ACP_AIC** obtient le deuxième meilleur score.  
Toutes les performances obtenues par **model_ACP_AIC** sont dans la moyenne haute des autres modèles.

## Approche bayésienne

En raison du temps de calcul, nous utiliserons le dataframe **d_train_algo**.  
Nous laissons la fonction définir les priors par défaut.
```{r modèle bayesien, results='hide'}
# Capturer l'horodatage de début
start_time <- Sys.time()

# Ajuster le modèle de régression logistique bayésienne
model_bayes <- stan_glm(pluie.demain ~ ., 
                        data = d_train_algo, 
                        family = binomial(link = "logit"), 
                        chains = 4, 
                        iter = 2000, 
                        seed = 100) 

# Capturer l'horodatage de fin
end_time <- Sys.time()
# Calculer la différence en temps
time_taken <- end_time - start_time
# Afficher le temps de calcul
print(time_taken)
```
```{r Infos model_bayes}
# Tracer les diagnostics du modèle
posterior <- as.array(model_bayes)
color_scheme_set("blue")
mcmc_trace(posterior)

# Extraire les coefficients
coef(model_bayes)
```

Nous n'utilisons pas le critère AIC pour un modèle bayésien.  
D'autres critères propres à cette approche existent (DIC, WAIC ou LOO par exemple) mais ne permettraient pas une comparaison avec nos modèles fréquentistes.

Les traces sont bonnes, ce qui indique que les MCMC ont bien convergé (nombre d'itérations suffisant).  
Si nous comparons avec les valeurs des coefficients obtenus avec **model_algo** sur le même dataframe, nous constatons qu'elles sont assez proches:  
par exemple la variable **Snowfall.amount.raw.daily.sum..sfc.** obtient le coefficient **-0.3235**, contre **-0.3529132698** pour **model_bayes**. 

Évaluation de **model_bayes** par matrice de confusion:
```{r matrice de confusion model_bayes, echo=FALSE}
# Prédictions sur les données d'entraînement
pred_prob <- predict(model_bayes, newdata = d_train_algo, type = "response")
pred <- ifelse(pred_prob >= 0.5, "TRUE", "FALSE")
pred <- as.factor(pred)
pluie.demain <- as.factor(d_train_algo$pluie.demain)
conf_matrix <- confusionMatrix(pred, pluie.demain, positive ="TRUE")
print(conf_matrix)
```
Évaluation de **model_bayes** par cross-validation (10 plis):
```{r cv model_bayes, warning=FALSE, results='hide'} 
# Sortie cachée car très longue, affichage des résultats par le chunk suivant

# Capturer l'horodatage de début
start_time <- Sys.time()

set.seed(100) 
# validation croisée
cv_model <- cv.glm(d_train_algo, model_bayes, K = 10)  # K = nombre de plis pour la validation croisée

# Capturer l'horodatage de fin
end_time <- Sys.time()
# Calculer la différence en temps
time_taken <- end_time - start_time
# Afficher le temps de calcul
print(time_taken)
```
```{r affichage données chink précédent}
# Arrondir les valeurs à trois chiffres après la virgule
delta_rounded <- round(cv_model$delta, 3)

# Afficher les valeurs arrondies
print(delta_rounded)
```
Calculons l'AUC de **model_bayes**:
```{r AUC model_bayes, echo=FALSE, message=FALSE}
pred_prob <- predict(model_bayes, newdata = d_train_algo, type = "response")
roc_model_bayes <- roc(d_train$pluie.demain, pred_prob)
auc_model_bayes <- auc(roc_model_algo)
print(auc_model_bayes)
```
Si on le compare à **model_algo**, construit sur les mêmes variables, **model_bayes** obtient des performances très proches. Il obtient notamment un bon score dans l'estimation de l'erreur de prédiction par cross-validation (0.178).

## Comparaison des modèles 

```{r tableau récapitulatif des modèles, warning=FALSE}
# Définir les noms des modèles et les colonnes
model_names <- c("model_total", "model_step_AIC", "model_step_BIC", "model_algo", "model_select", "model_select_int", "model_ACP_complet", "model_ACP_13_int", "model_ACP_AIC	", "model_bayes")
metric_names <- c("AIC", "Accuracy", "Sensitivity", "Specificity", "erreur_CV", "AUC")

#Matrice des valeurs
metric_values <- matrix(
  c(
    1321.5, 0.7398, 0.7671, 0.7116, 0.189, 0.8176,
    1285.6, 0.7424, 0.7770, 0.7064, 0.182, 0.8138,
    1292.1, 0.7449, 0.7887, 0.6995, 0.182, 0.8082,
    1308.1, 0.7398, 0.7820, 0.6960, 0.186, 0.8005,
    1315.7, 0.7373, 0.7854, 0.6874, 0.188, 0.7953,
    1247.6, 0.7432, 0.7720, 0.7133, 0.177, 0.8241,
    1321.5, 0.7398, 0.7671, 0.7116, 0.189, 0.8176,
    1276.3,	0.7746, 0.8053, 0.7427, 0.185, 0.8418,
    1281.9, 0.7475, 0.7820, 0.7116, 0.180, 0.8138,
    "XXXX", 0.7356, 0.7704, 0.6995, 0.178, 0.8005),
  nrow = 10,
  byrow = TRUE)

# Convertir la matrice en data.frame
results_df <- data.frame(
  Model = model_names,
  AIC = metric_values[, 1],
  Accuracy = metric_values[, 2],
  Sensitivity = metric_values[, 3],
  Specificity = metric_values[, 4],
  erreur_CV = metric_values[, 5],
  AUC = metric_values[, 6])

# dégradés de couleur à chaque colonne, du meilleur (bleu) au plus mauvais (rouge)
formattable_df <- formattable(results_df, list(
  AIC = color_tile("deepskyblue", "darksalmon"), 
  Accuracy = color_tile("darksalmon", "deepskyblue"), 
  Sensitivity = color_tile("darksalmon", "deepskyblue"), 
  Specificity = color_tile("darksalmon", "deepskyblue"), 
  erreur_CV = color_tile("deepskyblue", "darksalmon"), 
  AUC = color_tile("darksalmon", "deepskyblue") 
))

formattable_df
```

Le tableau confirme que c'est bien **model_ACP_13_int** qui obtient dans l'ensemble les meilleurs résultats.

### Courbes ROC

Traçons les courbes ROC de tous les modèles sur un même graphique:
```{r courbe ROC, fig.height=9, fig.width=9}
plot(roc_model_total, col = "blue", main = "Courbes ROC de tous les modèles", lwd = 1)
plot(roc_model_step_AIC, col = "gold3", add = TRUE, lwd = 1)
plot(roc_model_step_BIC, col = "darksalmon", add = TRUE, lwd = 1)
plot(roc_model_algo, col = "seagreen4", add = TRUE, lwd = 1)
plot(roc_model_select, col = "deeppink3", add = TRUE, lwd = 1)
plot(roc_model_select_int, col = "khaki4", add = TRUE, lwd = 1)
plot(roc_model_ACP_complet, col = "palegreen", add = TRUE, lwd = 1)
plot(roc_model_ACP_13_int, col = "plum1", add = TRUE, lwd = 1)
plot(roc_model_ACP_AIC, col = "red", add = TRUE, lwd = 1)
plot(roc_model_bayes, col = "skyblue", add = TRUE, lwd = 1)

legend("bottomright", legend = c("model_total", "model_step_AIC", "model_step_BIC", "model_algo", "model_select", "model_select_int", "model_ACP_complet", "model_ACP_13_int", "model_ACP_AIC", "model_bayes"), col = c("blue", "gold3", "darksalmon", "seagreen4", "deeppink3", "khaki4", "palegreen","plum1", "red", "skyblue"), lwd = 2)
```

Toutes nos courbes ROC sont assez proches, seule celle de **model_ACP_13_int** se détache légèrement. 

Nous retenons 3 modèles: **model_ACP_13_int** pour ses performances générales, **model_ACP_13_int** pour son faible AIC et **model_step_AIC** pour... la sécurité.

## Prédictions sur le jeu de test

Importation des données du jeu de test:
```{r message=FALSE}
d_test <- read_csv("C:/Users/olivi/OneDrive - Université Paris-Dauphine/Documents ODD gram/Formation/Cours/Modèle Linéaire Généralisé/Projet/meteo.test.csv", 
    col_types = cols(...1 = col_skip(), Year = col_skip(), 
        Month = col_skip(), Day = col_skip(), 
        Hour = col_skip(), Minute = col_skip()))
```

### Prédictions avec **model_step_AIC**
```{r}
pred_model_step_AIC <- predict(model_step_AIC, newdata = d_test, type = "response") > 0.5
```

### Prédictions avec **model_select_int**

Commençons par appliquer les mêmes modifications que celles appliquées à notre dataframe d’entraînement:
```{r modifications dataframe de test}
d_test_new <- d_test %>%
  mutate(Medium.Cloud = rowMeans(select(., Medium.Cloud.Cover.daily.max..mid.cld.lay., Medium.Cloud.Cover.daily.mean..mid.cld.lay., )))  %>%
 select(-Medium.Cloud.Cover.daily.max..mid.cld.lay., -Medium.Cloud.Cover.daily.mean..mid.cld.lay.) %>%
  mutate(Low.Cloud = rowMeans(select(., Low.Cloud.Cover.daily.max..low.cld.lay., Low.Cloud.Cover.daily.mean..low.cld.lay., ))) %>%
 select(-Low.Cloud.Cover.daily.max..low.cld.lay., -Low.Cloud.Cover.daily.mean..low.cld.lay.)%>%
  mutate(High.Cloud = rowMeans(select(., High.Cloud.Cover.daily.mean..high.cld.lay., High.Cloud.Cover.daily.max..high.cld.lay., )))%>%
 select(-High.Cloud.Cover.daily.mean..high.cld.lay., -High.Cloud.Cover.daily.max..high.cld.lay.)%>%
  mutate(Total.Cloud = rowMeans(select(., Total.Cloud.Cover.daily.max..sfc., Total.Cloud.Cover.daily.mean..sfc., )))%>%
select(-Total.Cloud.Cover.daily.max..sfc., -Total.Cloud.Cover.daily.mean..sfc.)

pred_model_select_int <- predict(model_select_int, newdata = d_test_new, type = "response") > 0.5
```

### Prédicitons avec **model_ACP_13_int**
```{r Prédiciton avec **model_ACP_13_int}
# Transformer les données de test avec PCA en utilisant les mêmes transformations que celles appliquées aux données d'entraînement 
X_test_pca <- predict(pca_result, newdata = d_test)

# Sélectionner les 13 premières CP 
X_test_pca_13 <- X_test_pca[, 1:13]

# Créer un nouveau data.frame avec les composantes principales
data_test_pca_13 <- data.frame(X_test_pca_13)

# prédictions avec le modèle ACP
pred_model_ACP_13_int <- predict(model_ACP_13_int, newdata = data_test_pca_13, type = "response") > 0.5
```

### Comparaison des prédictions des trois modèles
```{r Comparaisons des prédictions des trois modèles}
# data.frame avec les combinaisons de valeurs
df <- data.frame(
  Vector1 = pred_model_step_AIC,
  Vector2 = pred_model_select_int,
  Vector3 = pred_model_ACP_13_int
)
# ajouter une colonne pour les combinaisons
df$comb <- apply(df, 1, function(row) paste(row, collapse = "-"))

# Compter les occurrences de chaque combinaison
counts <- as.data.frame(table(df$comb))

ggplot(counts, aes(x = Var1, y = Freq, fill = Var1)) +
  geom_bar(stat = "identity") +
  labs(x = "Résultats croisés", y = "Fréquences") +
  scale_x_discrete(labels = NULL) +  
  theme_minimal()
```


Très majoritairement, les réponses sont identiques sur les trois modèles.

### Prédictions finales

Faisons les moyennes des réponses des trois modèles retenus de cette manière: 

- si FALSE apparaît 3 fois -> réponse FALSE
- si FALSE apparaît 2 fois -> réponse FALSE
- si TRUE apparaît 3 fois  -> réponse TRUE
- si TRUE apparaît 2 fois  -> réponse TRUE

```{r }
# matrice avec les vecteurs
mat <- cbind(pred_model_step_AIC, pred_model_select_int, pred_model_ACP_13_int)

# le vecteur combiné
pred_model_final <- rowSums(mat) == 2 | rowSums(mat) == 3

# Compte les occurrences de TRUE et FALSE
count_true <- sum(pred_model_final)
count_false <- length(pred_model_final) - count_true

# data frame avec le vecteur combiné
df_comb <- data.frame(pred_model_final)

# graphique
ggplot(df_comb, aes(x = factor(pred_model_final), fill = factor(pred_model_final))) +
  geom_bar(stat = "count") +
  labs(x = paste("Résultat final \n", count_false, "FALSE /", count_true, "TRUE"),
       y = "Fréquence", fill = "Résultat final") +
  ggtitle("Diagramme des résultats finaux") +
  theme_minimal()
```


Notre prédiction finale est de 154 jours avec pluie, 136 sans pluie.

Ajoutons nos prédictons dans un nouveau dataframe **d_test_pred** et créons un fichier **"meteo.test_pred.csv"**.
```{r message=FALSE, warning=FALSE}
d_test_pred <- read_csv("C:/Users/olivi/OneDrive - Université Paris-Dauphine/Documents ODD gram/Formation/Cours/Modèle Linéaire Généralisé/Projet/meteo.test.csv")
d_test_pred$pluie.demain <- pred_model_final
write.csv(d_test_pred, file = "meteo.test_pred.csv", row.names = FALSE)
```

## Conclusion 

Nous avons essayé plusieurs approches et nous constatons des différences de performances qui restent globalement faibles (0.7746 de précision pour le modèle le plus performant contre 0.7356 pour le moins performant).  
Nous avons pu constater l'importance d'incorporer les interactions dans les calculs (le "meilleur" modèle inclut des interactions).  
D'autres modèles pourraient être developpés en mixant plusieurs approches, comme par exemple: appliquer le critère BIC sur les composantes principales. 
Une méthode efficace mais très coûteuse en calculs et en temps d'analyse des résultats serait de faire une sélection de variables (par critère AIC par exemple) en incorporant toutes les interactions de toutes les variables. Cela maximiserait l'utilisation de l'information contenue dans le jeu de données.  
La meilleure démarche à adopter dépend de l'objectif que l'on poursuit. Ici nous avons privilégié la qualité des prédictions, sans volonté explicative. Si l'on souhaite au contraire comprendre pourquoi il pleuvra ou non le jour suivant, et non plus chercher uniquement à le prédire, on privilégiera l'analyse des variables retenues et leur coefficient.  
Autre remarque, nous avons choisi comme seuil la probabilité 0.5. Selon l’intentionnalité, on pourrait retenir d'autres valeurs (si, par exemple, on souhaitait prévoir l'opportunité d'une course en montagne le lendemain, on chercherait surtout à éviter des faux négatifs et on choisirait un seuil plus bas).  

Pour l'approche bayésienne, nous avons laissé la fonction définir des priors par défaut. On aurait pu définir les priors en fonction de la distribution observée des variables. Dans le cas d'une analyse similaire mais effectuée par un expert du domaine observé, l'approche bayésienne gagnerait en pertinence: les priors pourraient être définis en fonction des connaissances à priori de l'expert sur chaque variable disponible.  
Un modèle bayésien pourrait aussi sans doute obtenir des performances supérieures à une approche fréquentiste dans le cas d'un jeu de données limité en nombre d'observations (là encore, à condition de bien définir les priors).   

Autre approche non développée ici car contraire au cahier des charges: nous aurions pu intégrer les variables de date dans la régression (mois, années et la numérotaion). Par curiosité, nous avons commencé à développer des modèles allant dans ce sens, et les résultats étaient très prometteurs.  

Nous avons essayé un modèle **probit** mais ne l'avons pas présenté ici, les performances obtenues étant en tout point comparables à celles du modèle **logit**.






<a id="ref1"></a>
**Récapitulatif des dataframes utilisés:**

- **d_train** : jeu de données d’entraînement sans les variables inutilisées (**Year**, **Month**, **Day**, **Hour** et **Minute**), soit 40 variables explicatives.  
- **d_train_algo** : jeu de données avec les variables sélectionnées par colinéarité, p-valeurs et ANOVA successives, soit 23 variables explicatives.  
- **d_train_new** : datframe incluant les variables modifiées "à la main".  
- **data_pca** : dataframe de toutes les composantes principales, soit 40 CP.  
- **data_pca_13** : dataframe des 13 premières composantes principales.  

**Récapitulatif des modèles utilisés:**

- **model_total** : modèle complet avec toutes les données (dataframe **d_train**).  
- **model_step_AIC** : modèle avec les variables sélectionnées par la fonction **step()** selon le critère **AIC** sur le dataframe **d_train**.
- **model_step_BIC** : modèle avec les variables sélectionnées par la fonction **step()** selon le critère **BIC** sur le dataframe **d_train**.
- **model_algo** : modèle sur **d_train_algo**.  
- **model_select** : modèle avec variables sélectionnées "à la main".  
- **model_select_int** : même variables que **model_select** incluant les interactions, sélection via **step()** critère AIC.   
- **model_ACP_complet** : modèle avec toutes les composantes principales sur **data_pca**.  
- **model_ACP_13_int** : modèle avec les 13 premières CP sur **data_pca** avec interactions, sélection par la fonction **step()** critère **AIC**.
- **model_ACP_step** :  modèle avec les CP sélectionnées par la fonction **step()** selon le critère **AIC** sur le dataframe **data_pca**.
- **model_bayes** : modèle bayésien sur **d_train_algo**.  



 

 
